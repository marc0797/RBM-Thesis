{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bde057b",
   "metadata": {},
   "source": [
    "# Implementation of a Supervised Restricted Boltzmann Machine (SRBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d092e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random\n",
    "using DelimitedFiles\n",
    "using Printf\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116743b",
   "metadata": {},
   "source": [
    "## Definition of structs\n",
    "\n",
    "We define two structs that will help keep the code more organized and easier to maintain. By using a struct for the architecture of the RBM as well as one for the hyperparameters, we can easily pass them as arguments to functions.\n",
    "Additionally, we could define default values for some of the fields, which would make it easier to modify and experiment with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.@kwdef mutable struct SRBM{T<:AbstractFloat}\n",
    "    num_visible   :: Int\n",
    "    num_hidden    :: Int\n",
    "    num_labels    :: Int\n",
    "    W             :: Matrix{T} = 0.01*(randn(num_visible+num_labels,num_hidden) .- 0.0)\n",
    "    b             :: Vector{T} = zeros(num_visible+num_labels)\n",
    "    c             :: Matrix{T} = zeros(num_hidden,num_labels)\n",
    "end;\n",
    "\n",
    "Base.@kwdef mutable struct hyperparameters{T<:AbstractFloat}\n",
    "    learning_rate :: T         = 0.001\n",
    "    weight_decay  :: T         = 0.000001\n",
    "    momentum      :: T         = 0.9\n",
    "    batch_size    :: Int       = 100\n",
    "    num_epochs    :: Int       = 1000\n",
    "    CDK           :: Int       = 1\n",
    "    skip          :: Int       = 1\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57064e3",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "For neurons that take values $(\\sigma,z) \\in \\{-1,1\\}^{N \\times K}$, the activation function becomes $\\frac{1}{2}(1 + tanh(x))$ instead of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c740a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function activation(x)\n",
    "    return 0.5*(1.0 + tanh.(x));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f93454",
   "metadata": {},
   "source": [
    "## Expectation function\n",
    "In the case of binary neurons $\\{0,1\\}$ it is the sigmoid, but for binary neurons $\\{-1,1\\}$ it is the hyperbolic tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function expectation(x)\n",
    "    return tanh.(x)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbf7eb",
   "metadata": {},
   "source": [
    "## Sampling procedures\n",
    "When calculating the gradients, we need to use Gibbs sampling in order to approximate the sum over all the state space. For that, we include 2 different functions: one for sampling hidden states, and the other for visible states with additional bits that encode the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c65d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_hidden(p)\n",
    "    h = p .> rand(size(p,1),size(p,2));\n",
    "    return 2.0*h .- 1.0;\n",
    "end;\n",
    "\n",
    "function sample_visible(rbm::SRBM,p,y)\n",
    "    v  = p .> rand(size(p,1),size(p,2));\n",
    "    v[rbm.num_visible+1:end,:] = Id[:,y];\n",
    "    return 2.0*v .- 1.0;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878acdb3",
   "metadata": {},
   "source": [
    "## Calculating the gradients\n",
    "One can use contrastive divergence to estimate the gradient of the log-likelihood of the model with respect to the parameters, and then update them in the direction of the negative gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d77643",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cd_gradients(rbm::SRBM, V0::Matrix{T}, y::Vector{Int}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    # Positive phase\n",
    "    PH_V0  = expectation.(rbm.W'*V0 + rbm.c[:,y]);             # (Nh,b)\n",
    "    W_pos  = V0*PH_V0';                                        # (Nv+Nl,Nh)\n",
    "    b_pos  = V0;                                               # (Nv+Nl,b)\n",
    "    c_pos  = PH_V0;                                            # (Nh,b)\n",
    "    \n",
    "    # k-step Contrastive Divergence (CD-k)\n",
    "    VK     = copy(V0);                                         # (Nv+Nl,b)\n",
    "    for _ in 1:parameters.CDK\n",
    "        PH_V   = activation.(rbm.W'*VK + rbm.c[:,y]);          # (Nh,b)\n",
    "        HK     = sample_hidden(PH_V);                          # (Nh,b)\n",
    "        PV_H   = activation.(rbm.W*HK .+ rbm.b);               # (Nv+Nl,b)\n",
    "        VK     = sample_visible(rbm,PV_H,y);                   # (Nv+Nl,b)\n",
    "    end\n",
    "    \n",
    "    # Negative phase\n",
    "    PH_VK  = expectation.(rbm.W'*VK + rbm.c[:,y]);             # (Nh,b)\n",
    "    W_neg  = VK*PH_VK';                                        # (Nv+Nl,Nh)\n",
    "    b_neg  = VK;                                               # (Nv+Nl,b)\n",
    "    c_neg  = PH_VK;                                            # (Nh,b)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    dW     = (W_pos - W_neg)/parameters.batch_size;            # (Nv+Nl,Nh)\n",
    "    db     = (b_pos - b_neg)/parameters.batch_size;            # (Nv+Nl,b)\n",
    "    dc     = (c_pos - c_neg)/parameters.batch_size;            # (Nh,b)\n",
    "    \n",
    "    return dW, db, dc;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfba92",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "With our function that estimates the gradients, we only need to implement the iterative process over all epochs in which each step looks at a batch of a certain size, computes the gradients with respect to that batch and updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfdfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_srbm(rbm::SRBM, TSet::Matrix{T}, labels::Vector{Int}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    # Prepare values for creating the batches\n",
    "    NTSet       = size(TSet,2);\n",
    "    indices     = collect(1:NTSet);\n",
    "    num_batches = Int(cld(NTSet,parameters.batch_size));\n",
    "    \n",
    "    # Define matrix with labels mapped to -1,1\n",
    "    Identity    = 2.0*Id .- 1.0;\n",
    "    \n",
    "    # Initialize the velocities\n",
    "    vW     = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "    vb     = zero(rbm.b);                                       # (Nv,1)\n",
    "    vc     = zero(rbm.c);                                       # (Nh,Nl)\n",
    "    \n",
    "    # Start the main loop\n",
    "    for epoch in 1:parameters.num_epochs\n",
    "        shuffle!(indices)\n",
    "        \n",
    "        # Initialize the change in parameters\n",
    "        dW = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "        db = zero(rbm.b);                                       # (Nv,1)\n",
    "        dc = zero(rbm.c);                                       # (Nh,Nl)\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch in 1:num_batches\n",
    "            # Get indices of batch\n",
    "            batch_indices = indices[(batch-1)*parameters.batch_size+1:min(batch*parameters.batch_size,NTSet)];\n",
    "            \n",
    "            # Get labels and batches of data\n",
    "            y  = labels[batch_indices];\n",
    "            V0 = [TSet[:,batch_indices] ; Identity[:,y]];\n",
    "            dW, db_i, dc_i = cd_gradients(rbm, V0, y, parameters);\n",
    "            \n",
    "            # Sum change in bias\n",
    "            db = sum(db_i,dims=2);\n",
    "            dc = dc_i*Id[:,y]';\n",
    "            \n",
    "            db = vec(db);\n",
    "            \n",
    "            # Add L2 regularization to punish large values\n",
    "            dW             -= parameters.weight_decay*rbm.W;\n",
    "            db             -= parameters.weight_decay*rbm.b;\n",
    "            dc             -= parameters.weight_decay*rbm.c;\n",
    "            \n",
    "            # Update velocities\n",
    "            vW              = parameters.momentum*vW .+ (1.0 - parameters.momentum)*dW;\n",
    "            vb              = parameters.momentum*vb .+ (1.0 - parameters.momentum)*db;\n",
    "            vc              = parameters.momentum*vc .+ (1.0 - parameters.momentum)*dc;\n",
    "            \n",
    "            # Update parameters\n",
    "            rbm.W          += parameters.learning_rate*vW;\n",
    "            rbm.b          += parameters.learning_rate*vb;\n",
    "            rbm.c          += parameters.learning_rate*vc;\n",
    "        end\n",
    "    end\n",
    "end;\n",
    "\n",
    "function train_srbm(\n",
    "        rbm::SRBM, \n",
    "        TSet::Matrix{T}, \n",
    "        labels::Vector{Int}, \n",
    "        parameters::hyperparameters, \n",
    "        Test::Matrix{T}, \n",
    "        test_labels::Vector{Int}, \n",
    "        P_ARC::Vector{T}, \n",
    "        P_EX::Vector{T},\n",
    "        avg_m::Vector{T},\n",
    "        avg_n::Vector{T}\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Prepare values for creating the batches\n",
    "    NTSet       = size(TSet,2);\n",
    "    indices     = collect(1:NTSet);\n",
    "    num_batches = Int(cld(NTSet,parameters.batch_size));\n",
    "    \n",
    "    # Define matrix with labels mapped to -1,1\n",
    "    Identity    = 2.0*Id .- 1.0;\n",
    "    \n",
    "    # Initialize the velocities\n",
    "    vW     = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "    vb     = zero(rbm.b);                                       # (Nv,1)\n",
    "    vc     = zero(rbm.c);                                       # (Nh,Nl)\n",
    "    \n",
    "    iter = 1;\n",
    "    #previous = rand([-1.0,1.0], rbm.num_visible, rbm.num_labels);\n",
    "    # Start the main loop\n",
    "    for epoch in 1:parameters.num_epochs\n",
    "        shuffle!(indices)\n",
    "        \n",
    "        # Initialize the change in parameters\n",
    "        dW = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "        db = zero(rbm.b);                                       # (Nv,1)\n",
    "        dc = zero(rbm.c);                                       # (Nh,Nl)\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch in 1:num_batches\n",
    "            # Get indices of batch\n",
    "            batch_indices = indices[(batch-1)*parameters.batch_size+1:min(batch*parameters.batch_size,NTSet)];\n",
    "            \n",
    "            # Get labels and batches of data\n",
    "            y  = labels[batch_indices];\n",
    "            V0 = [TSet[:,batch_indices] ; Identity[:,y]];\n",
    "            dW, db_i, dc_i = cd_gradients(rbm, V0, y, parameters);\n",
    "            \n",
    "            # Sum change in bias\n",
    "            db = sum(db_i,dims=2);\n",
    "            dc = dc_i*Id[:,y]';\n",
    "            \n",
    "            db = vec(db);\n",
    "            \n",
    "            # Add L2 regularization to punish large values\n",
    "            dW             -= parameters.weight_decay*rbm.W;\n",
    "            db             -= parameters.weight_decay*rbm.b;\n",
    "            dc             -= parameters.weight_decay*rbm.c;\n",
    "            \n",
    "            # Update velocities\n",
    "            vW              = parameters.momentum*vW .+ (1.0 - parameters.momentum)*dW;\n",
    "            vb              = parameters.momentum*vb .+ (1.0 - parameters.momentum)*db;\n",
    "            vc              = parameters.momentum*vc .+ (1.0 - parameters.momentum)*dc;\n",
    "            \n",
    "            # Update parameters\n",
    "            rbm.W          += parameters.learning_rate*vW;\n",
    "            rbm.b          += parameters.learning_rate*vb;\n",
    "            rbm.c          += parameters.learning_rate*vc;\n",
    "        end\n",
    "        \n",
    "        if epoch % parameters.skip == 0\n",
    "            iter += 1;\n",
    "            # Calculate classification probability \n",
    "            dum, P_ARC[iter]                   = accuracy(rbm,Test,test_labels);\n",
    "            dum, P_EX[iter]                    = accuracy(rbm,TSet,labels);\n",
    "            # Calculate overlaps\n",
    "            avg_m[iter], avg_n[iter]           = overlapping(rbm,Test,test_labels,TSet,labels);\n",
    "            #previous = final;\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909709f",
   "metadata": {},
   "source": [
    "## Useful functions for probability calculation\n",
    "We define some numerically robust functions in order to calculate the marginal probability of a visible state with _prob, and a softmax which we will use for accuracy testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6993255",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_sum_exp(u::AbstractVecOrMat, v::AbstractVecOrMat)\n",
    "    maxim(a,b) = (a > b) ? a : b;\n",
    "    max        = maxim.(u,v);\n",
    "    return max + log.(exp.(u - max) + exp.(v - max));\n",
    "end;\n",
    "\n",
    "function _prob(rbm::SRBM,x::AbstractVecOrMat, labels::Vector{Int})\n",
    "    return rbm.b'*x + sum(log_sum_exp(-rbm.c[:,labels] .- rbm.W'*x, rbm.c[:,labels] .+ rbm.W'*x),dims=1);\n",
    "end;\n",
    "\n",
    "function softmax(X::AbstractVecOrMat{T}, dim::Integer, theta::AbstractFloat=1.0)::AbstractVecOrMat where T <: AbstractFloat\n",
    "    #abstract exponentiation function, subtract max for numerical stability and scale by theta\n",
    "    _exp(x::AbstractVecOrMat, θ::AbstractFloat) = exp.((x .- maximum(x)) * θ);\n",
    "    \n",
    "    #softmax algorithm expects stablized eponentiated e\n",
    "    _sftmax(e::AbstractVecOrMat, d::Integer) = (e ./ sum(e, dims = d));\n",
    "    \n",
    "    _sftmax(_exp(X,theta), dim)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529623d",
   "metadata": {},
   "source": [
    "## Accuracy function\n",
    "The accuracy function takes a matrix of data, that can be the validation set or the test set, and computes the ratio between correct predictions and total examples,as well as the mean probability of assigning the correct label given an example over all the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(rbm::SRBM, data::Matrix{T}, labels::Vector{Int}) where T<:AbstractFloat\n",
    "    num_examples = size(data,2);\n",
    "    num_correct  = 0;\n",
    "    mean         = 0.0;\n",
    "    # Define matrix with labels\n",
    "    Identity     = 2.0*Id .- 1.0;\n",
    "    \n",
    "    for i in 1:num_examples\n",
    "        # Compute probability of each label given the data\n",
    "        py_v            = _prob(rbm,[repeat(data[:,i],inner=(1,rbm.num_labels)) ; Identity],collect(1:rbm.num_labels));\n",
    "        \n",
    "        # Predict the label with the highest probability\n",
    "        softy_v         = vec(softmax(py_v,2));\n",
    "        predicted_label = argmax(softy_v);\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        num_correct += 1*(predicted_label == labels[i]);\n",
    "        \n",
    "        # Calculate mean\n",
    "        mean        += softy_v[labels[i]]/num_examples;\n",
    "    end\n",
    "    \n",
    "    return num_correct/num_examples, mean\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bc433",
   "metadata": {},
   "source": [
    "## New state proposal function\n",
    "We need a function that generates randomly a new state given an original one for our Metropolis sampling procedure. This flip proposal function takes the input and randomly flips a segment of random length starting at any point within the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "function flip_proposal(x::AbstractVector{T}, min_length::Int=10) where T<:AbstractFloat    \n",
    "    # Get dimensions of input\n",
    "    n = length(x);\n",
    "    # Random starting point\n",
    "    k     = rand(1:n);\n",
    "    # Random length, capped to min_length\n",
    "    l     = rand(1:min(min_length, n .- k .+ 1));\n",
    "    # Copy and flip elements\n",
    "    x_new = copy(x);\n",
    "    x_new[k:k+l-1] .= -x[k:k+l-1];\n",
    "    return x_new\n",
    "end;\n",
    "\n",
    "function flip_proposal(x::AbstractMatrix{T}, min_length::Int=10) where T<:AbstractFloat\n",
    "    return hcat([flip_proposal(x[:,j], min_length) for j in 1:size(x,2)]...);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139d8dc",
   "metadata": {},
   "source": [
    "## Overlap function\n",
    "We need a function that calculates the overlap between the expected value of the visible layer when fixing a label and the original archetype related to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67271306",
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_binary_matrix(N::Int)\n",
    "    ncols = 2^N\n",
    "    nrows = N\n",
    "    binary_matrix = zeros(Int8, nrows, ncols)\n",
    "\n",
    "    for j in 1:ncols\n",
    "        i = j-1\n",
    "        for k in 1:N\n",
    "            binary_matrix[k, j] = (i >> (N-k)) & 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return binary_matrix\n",
    "end\n",
    "\n",
    "function overlapping(\n",
    "        rbm::SRBM, \n",
    "        data::Matrix{T}, \n",
    "        labels::Vector{Int}, \n",
    "        train_data::Matrix{T}, \n",
    "        train_labels::Vector{Int}\n",
    ") where T<:AbstractFloat\n",
    "    # Check data and labels have same dimensions\n",
    "    @assert size(data,2) == length(labels)\n",
    "    @assert size(train_data,2) == length(train_labels)\n",
    "    \n",
    "    Nl        = rbm.num_labels;\n",
    "    Nv        = rbm.num_visible;\n",
    "    M         = Int(length(train_labels)/length(labels));\n",
    "    \n",
    "    # Define matrix with labels;\n",
    "    Identity  = 2.0*Id .- 1.0;                                                      # (Nl,Nl)\n",
    "    \n",
    "    # Generate matrix with all states (only for N <= 20)\n",
    "    @assert Nv <= 20\n",
    "    XAll      = 2*generate_binary_matrix(Nv) .- 1;\n",
    "    max_size  = size(XAll,2);\n",
    "    \n",
    "    # Initialize expected value of visible state given the label\n",
    "    m_visible = zeros(Nv,Nl);                                                       # (Nv,Nl)\n",
    "    \n",
    "    for i in 1:Nl\n",
    "        # Repeat labels for all possible states\n",
    "        L_C             = fill(labels[i],max_size);                                 # (2ᴺᵛ,1)\n",
    "        \n",
    "        # Calculate joint probability of a state with i-th label\n",
    "        P_C             = _prob(rbm, [XAll ; Identity[:,L_C]], L_C);                # (1,2ᴺᵛ)\n",
    "        \n",
    "        # Calculate the expected value of the visible state given the i-th label\n",
    "        m_visible[:,i]  = sum(XAll.*P_C,dims=2)./sum(P_C);\n",
    "    end\n",
    "    \n",
    "    n_visible = repeat(m_visible, inner=(1,M));\n",
    "    \n",
    "    m_prod = [dot(m_visible[:,i],data[:,i])       for i in 1:Nl  ]/Nv;\n",
    "    n_prod = [dot(n_visible[:,i],train_data[:,i]) for i in 1:Nl*M]/Nv;\n",
    "    \n",
    "    return sum(m_prod)./length(m_prod), sum(n_prod)./length(n_prod);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a6778",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "Hyperparameter tuning consists in exploring a grid of parameters and training our SRBM with them in order to find the combination that yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using .Iterators\n",
    "\n",
    "function grid_search(\n",
    "        num_visible::Int,\n",
    "        num_labels::Int,\n",
    "        train_data::Matrix{T}, \n",
    "        train_labels::Vector{Int}, \n",
    "        validation_data::Matrix{T}, \n",
    "        validation_labels::Vector{Int}, \n",
    "        test_data::Matrix{T}, \n",
    "        test_labels::Vector{Int}\n",
    ") where T<:AbstractFloat\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rates           = [0.001, 0.005, 0.01, 0.05, 0.1];\n",
    "    weight_decays            = [0.00001, 0.0001, 0.001, 0.01, 0.1];\n",
    "    momentum_coefficients    = [0.0, 0.5, 0.9];\n",
    "    num_hidden_units         = [10, 20, 50, 100];\n",
    "\n",
    "    # Define grid of hyperparameters to search over\n",
    "    hyperparameter_grid      = product(learning_rates, weight_decays, momentum_coefficients, num_hidden_units);\n",
    "\n",
    "    # Train SRBM with each set of hyperparameters and evaluate on validation set\n",
    "    best_validation_accuracy = -Inf;\n",
    "    best_mean_probability    = -Inf;\n",
    "    best_hyperparameters     = (0.0, 0.0, 0.0, 0);\n",
    "    for hyperparameters_set in hyperparameter_grid\n",
    "        learning_rate, weight_decay, momentum_coefficient, num_hidden = hyperparameters_set;\n",
    "        \n",
    "        rbm = SRBM(num_visible = num_visible, num_hidden = num_hidden, num_labels = num_labels); \n",
    "        parameters           = hyperparameters(\n",
    "            learning_rate = learning_rate, \n",
    "            weight_decay = weight_decay, \n",
    "            momentum = momentum_coefficient, \n",
    "            batch_size = 10, \n",
    "            num_epochs = 50\n",
    "        );\n",
    "        \n",
    "        train_srbm(rbm,train_data,train_labels,parameters);\n",
    "        \n",
    "        \n",
    "        validation_accuracy, mean = accuracy(rbm, validation_data, validation_labels)\n",
    "        #println(\"Hyperparameters: \", hyperparameters_set, \", Validation accuracy: \", validation_accuracy)\n",
    "        if validation_accuracy >= best_validation_accuracy && mean > best_mean_probability\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_mean_probability    = mean\n",
    "            best_hyperparameters = hyperparameters_set\n",
    "            #println(\"Hyperparameters: \", hyperparameters_set, \", Validation accuracy: \", validation_accuracy)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return best_hyperparameters;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e29d3",
   "metadata": {},
   "source": [
    "## Training dataset generation\n",
    "We generate the dataset for training, validation and test sets inside a function for memory allocation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function archetype_dataset(\n",
    "        N::Int,\n",
    "        K::Int,\n",
    "        r::T,\n",
    "        MT::Int,\n",
    "        MV::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Calculate probability of keeping bit = 1.\n",
    "    p                 = (r + 1.0)*0.5;\n",
    "    \n",
    "    # Test set corresponds to the random archetypes\n",
    "    test_data         = 2.0*(rand(N,K) .< 0.5) .- 1.0;\n",
    "    test_labels       = collect(1:K);\n",
    "\n",
    "    # Validation set helps tune the hyperparameters with blurred examples (can have different size as training set)\n",
    "    validation_data   = repeat(test_data,inner=(1,MV)).*(2.0*(rand(N,K*MV) .< p) .- 1.0);\n",
    "    validation_labels = repeat(test_labels,inner=MV);\n",
    "\n",
    "    # Training set corresponds to the blurred examples\n",
    "    train_data        = repeat(test_data,inner=(1,MT)).*(2.0*(rand(N,K*MT) .< p) .- 1.0);\n",
    "    train_labels      = repeat(test_labels,inner=MT);\n",
    "    \n",
    "    return train_data, train_labels, validation_data, validation_labels, test_data, test_labels\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c401ccd",
   "metadata": {},
   "source": [
    "## The main code\n",
    "Finally, the main code that will accept the problem variables, as well as the datasets containing training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "function main(\n",
    "        num_visible::Int, \n",
    "        num_hidden::Int, \n",
    "        num_labels::Int, \n",
    "        quality_examples::T,\n",
    "        M_train::Int,\n",
    "        M_validation::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Generate training set\n",
    "    train_data, train_labels, validation_data, validation_labels, test_data, test_labels = archetype_dataset(\n",
    "        num_visible,\n",
    "        num_labels,\n",
    "        quality_examples,\n",
    "        M_train,\n",
    "        M_validation\n",
    "    );\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    #=\n",
    "    learning_rate, weight_decay, momentum, hidden_units = grid_search(\n",
    "        num_visible,\n",
    "        num_labels,\n",
    "        train_data, \n",
    "        train_labels, \n",
    "        validation_data, \n",
    "        validation_labels, \n",
    "        test_data, \n",
    "        test_labels\n",
    "    )\n",
    "    =#\n",
    "    # Initialize the random weights (for the default values mean = 0.0, deviation = 0.01 we can skip this part)\n",
    "    #mean      = 0;\n",
    "    #deviation = 0.01;\n",
    "    #W         = deviation*(randn(num_visible+num_labels, num_hidden) .- mean);\n",
    "    #b         = zeros(num_visible+num_labels);\n",
    "    #c         = zeros(num_hidden, num_labels);\n",
    "    \n",
    "    # Initialize RBM\n",
    "    rbm        = SRBM(\n",
    "        num_visible   = num_visible,\n",
    "        num_hidden    = num_hidden,\n",
    "        num_labels    = num_labels\n",
    "    );\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = hyperparameters(\n",
    "        learning_rate = 0.01,\n",
    "        weight_decay  = 0.00001,\n",
    "        momentum      = 0.9,\n",
    "        batch_size    = 50,\n",
    "        num_epochs    = 1000,\n",
    "        skip          = 10\n",
    "    ) \n",
    "    \n",
    "    #=\n",
    "    # Train SRBM with best set of hyperparameters on entire training set\n",
    "    num_elems         = cld(parameters.num_epochs,parameters.skip) + 1;\n",
    "    probs_archetypes  = zeros(num_elems);\n",
    "    probs_examples    = zeros(num_elems);\n",
    "    avg_m             = zeros(num_elems);\n",
    "    avg_n             = zeros(num_elems);\n",
    "    train_srbm(rbm,train_data,train_labels,parameters,test_data,test_labels,probs_archetypes,probs_examples,avg_m,avg_n)\n",
    "    =#\n",
    "    train_srbm(rbm,train_data,train_labels,parameters)\n",
    "    \n",
    "    # Export classification probabilities\n",
    "    # Calculate classification probability \n",
    "    ~, P_ARC          = accuracy(rbm,test_data,test_labels);\n",
    "    ~, P_EX           = accuracy(rbm,train_data,train_labels);\n",
    "    # Calculate overlaps\n",
    "    avg_m, avg_n      = overlapping(rbm,test_data,test_labels,train_data,train_labels);\n",
    "    return P_ARC, P_EX, avg_m, avg_n;  \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef428cba",
   "metadata": {},
   "source": [
    "## Performance of the RBM\n",
    "\n",
    "We define the number of visible units $N$ and the ranges in which the number of examples per archetype $M$ and their quality $r$ will vary.\n",
    "\n",
    "Then, we nest the loop with a hierarchy with $r$ first and then $M$. This does not affect the result in any way, this is just to organize better the output results.\n",
    "\n",
    "This code outputs four .txt files containing the classification probabilities and the overlaps $\\langle m \\rangle$ and $\\langle n \\rangle$ (averaged for 100 iterations). These files can be later used to plot the figures seen in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visible       = 20;\n",
    "num_hidden        = 50;\n",
    "num_labels        = 4;\n",
    "\n",
    "global const Id = Matrix{Float64}(I, num_labels, num_labels)\n",
    "\n",
    "M_validation      = 4;\n",
    "\n",
    "rvec              = [0.66 0.32 0.18 0.10 0.06 0.04];\n",
    "Mvec              = [1    2    4    8    16   32  ];\n",
    "\n",
    "probs_archetypes  = zeros(length(Mvec),length(rvec));\n",
    "probs_examples    = zeros(length(Mvec),length(rvec));\n",
    "overlap_m         = zeros(length(Mvec),length(rvec));\n",
    "overlap_n         = zeros(length(Mvec),length(rvec));\n",
    "\n",
    "\n",
    "maxit = 100;\n",
    "for ir in 1:length(rvec)\n",
    "    r = rvec[ir];\n",
    "    for im in 1:length(Mvec)\n",
    "        M_train = Mvec[im];\n",
    "        \n",
    "        P_ARC   = 0.0;\n",
    "        P_EX    = 0.0;\n",
    "        avg_m   = 0.0;\n",
    "        avg_n   = 0.0;        \n",
    "        \n",
    "        for i in 1:maxit\n",
    "            P_ARC_i, P_EX_i, avg_m_i, avg_n_i = main(num_visible,num_hidden,num_labels,r,M_train,M_validation);\n",
    "            P_ARC += P_ARC_i/maxit;\n",
    "            P_EX  += P_EX_i/maxit;\n",
    "            avg_m += avg_m_i/maxit;\n",
    "            avg_n += avg_n_i/maxit;\n",
    "        end\n",
    "        \n",
    "        probs_archetypes[im,ir]  = P_ARC;\n",
    "        probs_examples[im,ir]    = P_EX;\n",
    "        overlap_m[im,ir]         = avg_m;\n",
    "        overlap_n[im,ir]         = avg_n;\n",
    "        println(\"M = \", M_train, \", r = \", r, \". Done.\")\n",
    "    end\n",
    "end\n",
    "str = @sprintf(\"SRBM-PARC-N%d-avg%d.txt\",num_visible,maxit);\n",
    "writedlm(str,probs_archetypes);\n",
    "str = @sprintf(\"SRBM-PEX-N%d-avg%d.txt\",num_visible,maxit);\n",
    "writedlm(str,probs_examples);\n",
    "str = @sprintf(\"SRBM-avgm-N%d-avg%d.txt\",num_visible,maxit);\n",
    "writedlm(str,overlap_m);\n",
    "str = @sprintf(\"SRBM-avgn-N%d-avg%d.txt\",num_visible,maxit);\n",
    "writedlm(str,overlap_n);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
