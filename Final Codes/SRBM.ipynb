{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bde057b",
   "metadata": {},
   "source": [
    "# Implementation of a Supervised Restricted Boltzmann Machine (SRBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d092e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2df7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608fc7e",
   "metadata": {},
   "source": [
    "## Definition of structs\n",
    "\n",
    "We define two structs that will help keep the code more organized and easier to maintain. By using a struct for the architecture of the RBM as well as one for the hyperparameters, we can easily pass them as arguments to functions.\n",
    "Additionally, we could define default values for some of the fields, which would make it easier to modify and experiment with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.@kwdef mutable struct SRBM{T<:AbstractFloat}\n",
    "    num_visible   :: Int\n",
    "    num_hidden    :: Int\n",
    "    num_labels    :: Int\n",
    "    W             :: Matrix{T} = 0.01*(randn(num_visible+num_labels,num_hidden) .- 0.0)\n",
    "    b             :: Vector{T} = zeros(num_visible+num_labels)\n",
    "    c             :: Matrix{T} = zeros(num_hidden,num_labels)\n",
    "end;\n",
    "\n",
    "Base.@kwdef mutable struct hyperparameters\n",
    "    learning_rate :: Float64 = 0.001\n",
    "    weight_decay  :: Float64 = 0.000001\n",
    "    momentum      :: Float64 = 0.9\n",
    "    batch_size    :: Int     = 100\n",
    "    num_epochs    :: Int     = 1000\n",
    "    CDK           :: Int     = 1\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6864d7",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "For neurons that take values $(\\sigma,z) \\in \\{-1,1\\}^{N \\times K}$, the activation function becomes $\\frac{1}{2}(1 + tanh(x))$ instead of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c740a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function activation(x)\n",
    "    return 0.5*(1.0 + tanh.(x));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd26f1e",
   "metadata": {},
   "source": [
    "## Expectation function\n",
    "In the case of binary neurons $\\{0,1\\}$ it is the sigmoid, but for binary neurons $\\{-1,1\\}$ it is the hyperbolic tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function expectation(x)\n",
    "    return tanh.(x)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25da1a1",
   "metadata": {},
   "source": [
    "## Sampling procedures\n",
    "When calculating the gradients, we need to use Gibbs sampling in order to approximate the sum over all the state space. For that, we include 2 different functions: one for sampling hidden states, and the other for visible states with additional bits that encode the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c65d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_hidden(p)\n",
    "    h = p .> rand(size(p,1),size(p,2));\n",
    "    return 2.0*h .- 1.0;\n",
    "end;\n",
    "\n",
    "function sample_visible(rbm::SRBM,p,y)\n",
    "    v  = p .> rand(size(p,1),size(p,2));\n",
    "    v[rbm.num_visible+1:end,:] = Id[:,y];\n",
    "    return 2.0*v .- 1.0;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002a5eb",
   "metadata": {},
   "source": [
    "## Calculating the gradients\n",
    "One can use contrastive divergence to estimate the gradient of the log-likelihood of the model with respect to the parameters, and then update them in the direction of the negative gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d77643",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cd_gradients(rbm::SRBM, V0::Matrix{T}, y::Vector{Int}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    # Positive phase\n",
    "    PH_V0  = expectation.(rbm.W'*V0 + rbm.c[:,y]);             # (Nh,b)\n",
    "    W_pos  = V0*PH_V0';                                        # (Nv+Nl,Nh)\n",
    "    b_pos  = V0;                                               # (Nv+Nl,b)\n",
    "    c_pos  = PH_V0;                                            # (Nh,b)\n",
    "    \n",
    "    # k-step Contrastive Divergence (CD-k)\n",
    "    VK     = copy(V0);                                         # (Nv+Nl,b)\n",
    "    for _ in 1:parameters.CDK\n",
    "        PH_V   = activation.(rbm.W'*VK + rbm.c[:,y]);          # (Nh,b)\n",
    "        HK     = sample_hidden(PH_V);                          # (Nh,b)\n",
    "        PV_H   = activation.(rbm.W*HK .+ rbm.b);               # (Nv+Nl,b)\n",
    "        VK     = sample_visible(rbm,PV_H,y);                   # (Nv+Nl,b)\n",
    "    end\n",
    "    \n",
    "    # Negative phase\n",
    "    PH_VK  = expectation.(rbm.W'*VK + rbm.c[:,y]);             # (Nh,b)\n",
    "    W_neg  = VK*PH_VK';                                        # (Nv+Nl,Nh)\n",
    "    b_neg  = VK;                                               # (Nv+Nl,b)\n",
    "    c_neg  = PH_VK;                                            # (Nh,b)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    dW     = (W_pos - W_neg)/parameters.batch_size;            # (Nv+Nl,Nh)\n",
    "    db     = (b_pos - b_neg)/parameters.batch_size;            # (Nv+Nl,b)\n",
    "    dc     = (c_pos - c_neg)/parameters.batch_size;            # (Nh,b)\n",
    "    \n",
    "    return dW, db, dc;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031168cf",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "With our function that estimates the gradients, we only need to implement the iterative process over all epochs in which each step looks at a batch of a certain size, computes the gradients with respect to that batch and updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfdfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_srbm(rbm::SRBM, TSet::Matrix{T}, labels::Vector{Int}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    # Prepare values for creating the batches\n",
    "    NTSet       = size(TSet,2);\n",
    "    indices     = collect(1:NTSet);\n",
    "    num_batches = Int(cld(NTSet,parameters.batch_size));\n",
    "    \n",
    "    # Define matrix with labels mapped to -1,1\n",
    "    Identity    = 2.0*Id .- 1.0;\n",
    "    \n",
    "    # Initialize the velocities\n",
    "    vW     = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "    vb     = zero(rbm.b);                                       # (Nv,1)\n",
    "    vc     = zero(rbm.c);                                       # (Nh,Nl)\n",
    "    \n",
    "    # Start the main loop\n",
    "    for epoch in 1:parameters.num_epochs\n",
    "        shuffle!(indices)\n",
    "        \n",
    "        # Initialize the change in parameters\n",
    "        dW = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "        db = zero(rbm.b);                                       # (Nv,1)\n",
    "        dc = zero(rbm.c);                                       # (Nh,Nl)\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch in 1:num_batches\n",
    "            # Get indices of batch\n",
    "            batch_indices = indices[(batch-1)*parameters.batch_size+1:min(batch*parameters.batch_size,NTSet)];\n",
    "            \n",
    "            # Get labels and batches of data\n",
    "            y  = labels[batch_indices];\n",
    "            V0 = [TSet[:,batch_indices] ; Identity[:,y]];\n",
    "            dW, db_i, dc_i = cd_gradients(rbm, V0, y, parameters);\n",
    "            \n",
    "            # Sum change in bias\n",
    "            db = sum(db_i,dims=2);\n",
    "            dc = dc_i*Id[:,y]';\n",
    "            \n",
    "            db = vec(db);\n",
    "            \n",
    "            # Add L2 regularization to punish large values\n",
    "            dW             -= parameters.weight_decay*rbm.W;\n",
    "            db             -= parameters.weight_decay*rbm.b;\n",
    "            dc             -= parameters.weight_decay*rbm.c;\n",
    "            \n",
    "            # Update velocities\n",
    "            vW              = parameters.momentum*vW .+ (1.0 - parameters.momentum)*dW;\n",
    "            vb              = parameters.momentum*vb .+ (1.0 - parameters.momentum)*db;\n",
    "            vc              = parameters.momentum*vc .+ (1.0 - parameters.momentum)*dc;\n",
    "            \n",
    "            # Update parameters\n",
    "            rbm.W          += parameters.learning_rate*vW;\n",
    "            rbm.b          += parameters.learning_rate*vb;\n",
    "            rbm.c          += parameters.learning_rate*vc;\n",
    "        end\n",
    "        \n",
    "        # Calculate classification probability \n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba3d16",
   "metadata": {},
   "source": [
    "## Useful functions for probability calculation\n",
    "We define some numerically robust functions in order to calculate the marginal probability of a visible state with __prob_, and a _softmax_ which we will use for accuracy testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6993255",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_sum_exp(u::AbstractVecOrMat, v::AbstractVecOrMat)\n",
    "    maxim(a,b) = (a > b) ? a : b;\n",
    "    max        = maxim.(u,v);\n",
    "    return max + log.(exp.(u - max) + exp.(v - max));\n",
    "end;\n",
    "\n",
    "function _prob(rbm::SRBM,x::AbstractVecOrMat)\n",
    "    return rbm.b'*x + sum(log_sum_exp(-rbm.c .- rbm.W'*x, rbm.c .+ rbm.W'*x),dims=1);\n",
    "end;\n",
    "\n",
    "function softmax(X::AbstractVecOrMat{T}, dim::Integer, theta::AbstractFloat=1.0)::AbstractVecOrMat where T <: AbstractFloat\n",
    "    #abstract exponentiation function, subtract max for numerical stability and scale by theta\n",
    "    _exp(x::AbstractVecOrMat, θ::AbstractFloat) = exp.((x .- maximum(x)) * θ);\n",
    "    \n",
    "    #softmax algorithm expects stablized eponentiated e\n",
    "    _sftmax(e::AbstractVecOrMat, d::Integer) = (e ./ sum(e, dims = d));\n",
    "    \n",
    "    _sftmax(_exp(X,theta), dim)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1509ff",
   "metadata": {},
   "source": [
    "## Accuracy function\n",
    "The accuracy function takes a matrix of data, that can be the validation set or the test set, and computes the ratio between correct predictions and total examples, as well as the mean probability of assigning the correct label given an example over all the input dataset. In other words, the classification probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(rbm::SRBM, data::Matrix{T}, labels::Vector{Int}) where T<:AbstractFloat\n",
    "    num_examples = size(data,2);\n",
    "    num_correct  = 0;\n",
    "    mean         = 0.0;\n",
    "    # Define matrix with labels\n",
    "    Identity     = 2.0*Id .- 1.0;\n",
    "    \n",
    "    for i in 1:num_examples\n",
    "        # Compute probability of each label given the data\n",
    "        py_v            = _prob(rbm,[repeat(data[:,i],inner=(1,rbm.num_labels)) ; Identity]);\n",
    "        \n",
    "        # Predict the label with the highest probability\n",
    "        softy_v         = vec(softmax(py_v,2));\n",
    "        predicted_label = argmax(softy_v);\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        num_correct += 1*(predicted_label == labels[i]);\n",
    "        \n",
    "        # Calculate mean\n",
    "        mean        += softy_v[labels[i]]/num_examples;\n",
    "    end\n",
    "    \n",
    "    return num_correct/num_examples, mean\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d68a8",
   "metadata": {},
   "source": [
    "## Overlap function\n",
    "We need a function that calculates the overlap between the expected value of the visible layer when fixing a label and the original archetype related to that label. It also calculates the overlap between the same expected visible configuration and the distorted examples so that we can compare both values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e191bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_binary_matrix(N::Int)\n",
    "    ncols = 2^N\n",
    "    nrows = N\n",
    "    binary_matrix = zeros(Int8, nrows, ncols)\n",
    "\n",
    "    for j in 1:ncols\n",
    "        i = j-1\n",
    "        for k in 1:N\n",
    "            binary_matrix[k, j] = (i >> (N-k)) & 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return binary_matrix\n",
    "end\n",
    "\n",
    "function overlapping(\n",
    "        rbm::SRBM, \n",
    "        data::Matrix{T}, \n",
    "        labels::Vector{Int}, \n",
    "        train_data::Matrix{T}, \n",
    "        train_labels::Vector{Int}\n",
    ") where T<:AbstractFloat\n",
    "    # Check data and labels have same dimensions\n",
    "    @assert size(data,2) == length(labels)\n",
    "    @assert size(train_data,2) == length(train_labels)\n",
    "    \n",
    "    Nl        = rbm.num_labels;\n",
    "    Nv        = rbm.num_visible;\n",
    "    M         = Int(length(train_labels)/length(labels));\n",
    "    \n",
    "    # Define matrix with labels;\n",
    "    Identity  = 2.0*Id .- 1.0;                                                      # (Nl,Nl)\n",
    "    \n",
    "    # Generate matrix with all states (only for N <= 20)\n",
    "    @assert Nv <= 20\n",
    "    XAll      = 2*generate_binary_matrix(Nv) .- 1;\n",
    "    max_size  = size(XAll,2);\n",
    "    \n",
    "    # Initialize expected value of visible state given the label\n",
    "    m_visible = zeros(Nv,Nl);                                                       # (Nv,Nl)\n",
    "    \n",
    "    for i in 1:Nl\n",
    "        # Repeat labels for all possible states\n",
    "        L_C             = fill(labels[i],max_size);                                 # (2^Nv,1)\n",
    "        \n",
    "        # Calculate joint probability of a state with i-th label\n",
    "        P_C             = _prob(rbm, [XAll ; Identity[:,L_C]], L_C);                # (1,2^Nv)\n",
    "        \n",
    "        # Calculate the expected value of the visible state given the i-th label\n",
    "        m_visible[:,i]  = sum(XAll.*P_C,dims=2)./sum(P_C);\n",
    "    end\n",
    "    \n",
    "    n_visible = repeat(m_visible, inner=(1,M));\n",
    "    \n",
    "    m_prod = [dot(m_visible[:,i],data[:,i])       for i in 1:Nl  ]/Nv;\n",
    "    n_prod = [dot(n_visible[:,i],train_data[:,i]) for i in 1:Nl*M]/Nv;\n",
    "    \n",
    "    return sum(m_prod)./length(m_prod), sum(n_prod)./length(n_prod);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ff537",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "Hyperparameter tuning consists in exploring a grid of parameters and training our SRBM with them in order to find the combination that yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using .Iterators\n",
    "\n",
    "function grid_search(\n",
    "        num_visible::Int,\n",
    "        num_labels::Int,\n",
    "        train_data::Matrix{T}, \n",
    "        train_labels::Vector{Int}, \n",
    "        validation_data::Matrix{T}, \n",
    "        validation_labels::Vector{Int}, \n",
    "        test_data::Matrix{T}, \n",
    "        test_labels::Vector{Int}\n",
    ") where T<:AbstractFloat\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rates           = [0.001, 0.005, 0.01, 0.05, 0.1];\n",
    "    weight_decays            = [0.00001, 0.0001, 0.001, 0.01, 0.1];\n",
    "    momentum_coefficients    = [0.0, 0.5, 0.9];\n",
    "    num_hidden_units         = [10, 20, 50, 100];\n",
    "\n",
    "    # Define grid of hyperparameters to search over\n",
    "    hyperparameter_grid      = product(learning_rates, weight_decays, momentum_coefficients, num_hidden_units);\n",
    "\n",
    "    # Train SRBM with each set of hyperparameters and evaluate on validation set\n",
    "    best_validation_accuracy = -Inf;\n",
    "    best_mean_probability    = -Inf;\n",
    "    best_hyperparameters     = (0.0, 0.0, 0.0, 0);\n",
    "    for hyperparameters_set in hyperparameter_grid\n",
    "        learning_rate, weight_decay, momentum_coefficient, num_hidden = hyperparameters_set;\n",
    "        \n",
    "        rbm = SRBM(num_visible = num_visible, num_hidden = num_hidden, num_labels = num_labels); \n",
    "        parameters           = hyperparameters(\n",
    "            learning_rate = learning_rate, \n",
    "            weight_decay = weight_decay, \n",
    "            momentum = momentum_coefficient, \n",
    "            batch_size = 10, \n",
    "            num_epochs = 100\n",
    "        );\n",
    "        \n",
    "        train_srbm(rbm,train_data,train_labels,parameters);\n",
    "        \n",
    "        \n",
    "        validation_accuracy, mean = accuracy(rbm, validation_data, validation_labels)\n",
    "        #println(\"Hyperparameters: \", hyperparameters_set, \", Validation accuracy: \", validation_accuracy)\n",
    "        if validation_accuracy >= best_validation_accuracy && mean > best_mean_probability\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_mean_probability    = mean\n",
    "            best_hyperparameters = hyperparameters_set\n",
    "            println(\"Hyperparameters: \", hyperparameters_set, \", Validation accuracy: \", validation_accuracy)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return best_hyperparameters;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b40d",
   "metadata": {},
   "source": [
    "## Training dataset generation\n",
    "We generate the random dataset for training, validation and test sets inside a function for memory allocation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function archetype_dataset(\n",
    "        N::Int,\n",
    "        K::Int,\n",
    "        r::T,\n",
    "        MT::Int,\n",
    "        MV::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Calculate probability of keeping bit = 1.\n",
    "    p                 = (r + 1.0)*0.5;\n",
    "    \n",
    "    # Test set corresponds to the random archetypes\n",
    "    test_data         = 2.0*(rand(N,K) .< 0.5) .- 1.0;\n",
    "    test_labels       = collect(1:K);\n",
    "\n",
    "    # Validation set helps tune the hyperparameters with blurred examples (can have different size as training set)\n",
    "    validation_data   = repeat(test_data,inner=(1,MV)).*(2.0*(rand(N,K*MV) .< p) .- 1.0);\n",
    "    validation_labels = repeat(test_labels,inner=MV);\n",
    "\n",
    "    # Training set corresponds to the blurred examples\n",
    "    train_data        = repeat(test_data,inner=(1,MT)).*(2.0*(rand(N,K*MT) .< p) .- 1.0);\n",
    "    train_labels      = repeat(test_labels,inner=MT);\n",
    "    \n",
    "    return train_data, train_labels, validation_data, validation_labels, test_data, test_labels\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747661d",
   "metadata": {},
   "source": [
    "## The main code\n",
    "The main code that will accept the problem variables, as well as the datasets containing training, validation and test sets. It calculates and prints both the classification accuracy and the overlaps (if $N_v <= 20$) for the training set (blurred examples) and the test set (archetypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "function main(\n",
    "        num_visible::Int, \n",
    "        num_hidden::Int, \n",
    "        num_labels::Int, \n",
    "        quality_examples::T,\n",
    "        M_train::Int,\n",
    "        M_validation::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Generate training set\n",
    "    train_data, train_labels, validation_data, validation_labels, test_data, test_labels = archetype_dataset(\n",
    "        num_visible,\n",
    "        num_labels,\n",
    "        quality_examples,\n",
    "        M_train,\n",
    "        M_validation\n",
    "    );\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    learning_rate, weight_decay, momentum, hidden_units = grid_search(\n",
    "        num_visible,\n",
    "        num_labels,\n",
    "        train_data, \n",
    "        train_labels, \n",
    "        validation_data, \n",
    "        validation_labels, \n",
    "        test_data, \n",
    "        test_labels\n",
    "    )\n",
    "    \n",
    "    # Initialize the random weights (for the default values mean = 0.0, deviation = 0.01 we can skip this part)\n",
    "    #mean      = 0;\n",
    "    #deviation = 0.01;\n",
    "    #W         = deviation*(randn(num_visible+num_labels, num_hidden) .- mean);\n",
    "    #b         = zeros(num_visible+num_labels);\n",
    "    #c         = zeros(num_hidden, num_labels);\n",
    "    \n",
    "    # Initialize RBM\n",
    "    rbm        = SRBM(\n",
    "        num_visible   = num_visible,\n",
    "        num_hidden    = hidden_units,\n",
    "        num_labels    = num_labels\n",
    "    );\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = hyperparameters(\n",
    "        learning_rate = learning_rate,\n",
    "        weight_decay  = weight_decay,\n",
    "        momentum      = momentum,\n",
    "        batch_size    = 50,\n",
    "        num_epochs    = 100\n",
    "    ) \n",
    "    \n",
    "    # Train SRBM with best set of hyperparameters on entire training set\n",
    "    train_srbm(rbm,train_data,train_labels,parameters)\n",
    "\n",
    "    # Evaluate performance on test set (classification mode)\n",
    "    test_accuracy, mean = accuracy(rbm, test_data, test_labels);\n",
    "    println(\"Test accuracy: \", test_accuracy);\n",
    "    println(\"Mean probability: \",mean);\n",
    "    \n",
    "    # Evaluate performance on test set (generative mode)\n",
    "    overlap             = overlapping(rbm, test_data, test_labels);\n",
    "    println(\"<m> overlap: \", overlap);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9985c",
   "metadata": {},
   "source": [
    "## Define problem variables\n",
    "\n",
    "Before calling the main function, we need to define the number of units that encode the training examples $N$, the number of hidden units $N_h$, the number of archetypes (or labels) $K$. Recall that the total number of visible units $N_v = N + K$.\n",
    "\n",
    "In order to generate the training set, we need the quality of the examples $r$ and the number of examples per archetype $M$. $M_{validation}$ is used to create a smaller dataset for the hyperparameter tuning function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem variables\n",
    "num_visible       = 200;\n",
    "num_hidden        = 4*num_visible;\n",
    "num_labels        = 4;\n",
    "\n",
    "# Training set variables\n",
    "quality_examples  = 0.25;\n",
    "M_train           = 5;\n",
    "M_validation      = 4;\n",
    "\n",
    "global const Id = Matrix{Float64}(I, num_labels, num_labels)\n",
    "\n",
    "main(num_visible,num_hidden,num_labels,quality_examples,M_train,M_validation);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
