{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bde057b",
   "metadata": {},
   "source": [
    "# Implementation of a Supervised Restricted Boltzmann Machine (SRBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d092e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Random\n",
    "using DelimitedFiles\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116743b",
   "metadata": {},
   "source": [
    "## Definition of structs\n",
    "\n",
    "We define two structs that will help keep the code more organized and easier to maintain. By using a struct for the architecture of the RBM as well as one for the hyperparameters, we can easily pass them as arguments to functions.\n",
    "Additionally, we could define default values for some of the fields, which would make it easier to modify and experiment with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.@kwdef mutable struct SRBM{T<:AbstractFloat}\n",
    "    num_visible   :: Int\n",
    "    num_hidden    :: Int\n",
    "    num_labels    :: Int\n",
    "    W             :: Matrix{T} = 0.01*(randn(num_visible+num_labels,num_hidden) .- 0.0)\n",
    "    b             :: Vector{T} = zeros(num_visible+num_labels)\n",
    "    c             :: Matrix{T} = zeros(num_hidden,num_labels)\n",
    "end;\n",
    "\n",
    "Base.@kwdef mutable struct hyperparameters{T<:AbstractFloat}\n",
    "    learning_rate :: T         = 0.001\n",
    "    weight_decay  :: T         = 0.000001\n",
    "    momentum      :: T         = 0.9\n",
    "    batch_size    :: Int       = 100\n",
    "    num_epochs    :: Int       = 1000\n",
    "    CDK           :: Int       = 1\n",
    "    skip          :: Int       = 1\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57064e3",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "For neurons that take values $(\\sigma,z) \\in \\{-1,1\\}^{N \\times K}$, the activation function becomes $\\frac{1}{2}(1 + tanh(x))$ instead of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c740a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function activation(x)\n",
    "    return 0.5*(1.0 + tanh.(x));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f93454",
   "metadata": {},
   "source": [
    "## Expectation function\n",
    "In the case of binary neurons $\\{0,1\\}$ it is the sigmoid, but for binary neurons $\\{-1,1\\}$ it is the hyperbolic tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function expectation(x)\n",
    "    return tanh.(x)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbf7eb",
   "metadata": {},
   "source": [
    "## Sampling procedures\n",
    "When calculating the gradients, we need to use Gibbs sampling in order to approximate the sum over all the state space. For that, we include 2 different functions: one for sampling hidden states, and the other for visible states with additional bits that encode the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c65d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_hidden(p)\n",
    "    h = p .> rand(size(p,1),size(p,2));\n",
    "    return 2.0*h .- 1.0;\n",
    "end;\n",
    "\n",
    "function sample_visible(rbm::SRBM,p,y)\n",
    "    v  = p .> rand(size(p,1),size(p,2));\n",
    "    v[rbm.num_visible+1:end,:] = Id[:,y];\n",
    "    return 2.0*v .- 1.0;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878acdb3",
   "metadata": {},
   "source": [
    "## Calculating the gradients\n",
    "One can use contrastive divergence to estimate the gradient of the log-likelihood of the model with respect to the parameters, and then update them in the direction of the negative gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d77643",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cd_gradients(rbm::SRBM, V0::Matrix{T}, y::Vector{Int}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    # Positive phase\n",
    "    PH_V0  = expectation.(rbm.W'*V0 + rbm.c[:,y]);             # (Nh,b)\n",
    "    W_pos  = V0*PH_V0';                                        # (Nv+Nl,Nh)\n",
    "    b_pos  = V0;                                               # (Nv+Nl,b)\n",
    "    c_pos  = PH_V0;                                            # (Nh,b)\n",
    "    \n",
    "    # k-step Contrastive Divergence (CD-k)\n",
    "    VK     = copy(V0);                                         # (Nv+Nl,b)\n",
    "    for _ in 1:parameters.CDK\n",
    "        PH_V   = activation.(rbm.W'*VK + rbm.c[:,y]);          # (Nh,b)\n",
    "        HK     = sample_hidden(PH_V);                          # (Nh,b)\n",
    "        PV_H   = activation.(rbm.W*HK .+ rbm.b);               # (Nv+Nl,b)\n",
    "        VK     = sample_visible(rbm,PV_H,y);                   # (Nv+Nl,b)\n",
    "    end\n",
    "    \n",
    "    # Negative phase\n",
    "    PH_VK  = expectation.(rbm.W'*VK + rbm.c[:,y]);             # (Nh,b)\n",
    "    W_neg  = VK*PH_VK';                                        # (Nv+Nl,Nh)\n",
    "    b_neg  = VK;                                               # (Nv+Nl,b)\n",
    "    c_neg  = PH_VK;                                            # (Nh,b)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    dW     = (W_pos - W_neg)/parameters.batch_size;            # (Nv+Nl,Nh)\n",
    "    db     = (b_pos - b_neg)/parameters.batch_size;            # (Nv+Nl,b)\n",
    "    dc     = (c_pos - c_neg)/parameters.batch_size;            # (Nh,b)\n",
    "    \n",
    "    return dW, db, dc;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfba92",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "With our function that estimates the gradients, we only need to implement the iterative process over all epochs in which each step looks at a batch of a certain size, computes the gradients with respect to that batch and updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfdfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_srbm(rbm::SRBM, TSet::Matrix{T}, labels::Vector{Int}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    # Prepare values for creating the batches\n",
    "    NTSet       = size(TSet,2);\n",
    "    indices     = collect(1:NTSet);\n",
    "    num_batches = Int(cld(NTSet,parameters.batch_size));\n",
    "    \n",
    "    # Define matrix with labels mapped to -1,1\n",
    "    Identity    = 2.0*Id .- 1.0;\n",
    "    \n",
    "    # Initialize the velocities\n",
    "    vW     = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "    vb     = zero(rbm.b);                                       # (Nv,1)\n",
    "    vc     = zero(rbm.c);                                       # (Nh,Nl)\n",
    "    \n",
    "    # Start the main loop\n",
    "    for epoch in 1:parameters.num_epochs\n",
    "        shuffle!(indices)\n",
    "        \n",
    "        # Initialize the change in parameters\n",
    "        dW = zero(rbm.W);                                       # (Nv+Nl,Nh)\n",
    "        db = zero(rbm.b);                                       # (Nv,1)\n",
    "        dc = zero(rbm.c);                                       # (Nh,Nl)\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch in 1:num_batches\n",
    "            # Get indices of batch\n",
    "            batch_indices = indices[(batch-1)*parameters.batch_size+1:min(batch*parameters.batch_size,NTSet)];\n",
    "            \n",
    "            # Get labels and batches of data\n",
    "            y  = labels[batch_indices];\n",
    "            V0 = [TSet[:,batch_indices] ; Identity[:,y]];\n",
    "            dW, db_i, dc_i = cd_gradients(rbm, V0, y, parameters);\n",
    "            \n",
    "            # Sum change in bias\n",
    "            db = sum(db_i,dims=2);\n",
    "            dc = dc_i*Id[:,y]';\n",
    "            \n",
    "            db = vec(db);\n",
    "            \n",
    "            # Add L2 regularization to punish large values\n",
    "            dW             -= parameters.weight_decay*rbm.W;\n",
    "            db             -= parameters.weight_decay*rbm.b;\n",
    "            dc             -= parameters.weight_decay*rbm.c;\n",
    "            \n",
    "            # Update velocities\n",
    "            vW              = parameters.momentum*vW .+ (1.0 - parameters.momentum)*dW;\n",
    "            vb              = parameters.momentum*vb .+ (1.0 - parameters.momentum)*db;\n",
    "            vc              = parameters.momentum*vc .+ (1.0 - parameters.momentum)*dc;\n",
    "            \n",
    "            # Update parameters\n",
    "            rbm.W          += parameters.learning_rate*vW;\n",
    "            rbm.b          += parameters.learning_rate*vb;\n",
    "            rbm.c          += parameters.learning_rate*vc;\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2909709f",
   "metadata": {},
   "source": [
    "## Useful functions for probability calculation\n",
    "We define some numerically robust functions in order to calculate the marginal probability of a visible state with _prob, and a softmax which we will use for accuracy testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6993255",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_sum_exp(u::AbstractVecOrMat, v::AbstractVecOrMat)\n",
    "    maxim(a,b) = (a > b) ? a : b;\n",
    "    max        = maxim.(u,v);\n",
    "    return max + log.(exp.(u - max) + exp.(v - max));\n",
    "end;\n",
    "\n",
    "function _prob(rbm::SRBM,x::AbstractVecOrMat, labels::Vector{Int})\n",
    "    return rbm.b'*x + sum(log_sum_exp(-rbm.c[:,labels] .- rbm.W'*x, rbm.c[:,labels] .+ rbm.W'*x),dims=1);\n",
    "end;\n",
    "\n",
    "function softmax(X::AbstractVecOrMat{T}, dim::Integer, theta::AbstractFloat=1.0)::AbstractVecOrMat where T <: AbstractFloat\n",
    "    #abstract exponentiation function, subtract max for numerical stability and scale by theta\n",
    "    _exp(x::AbstractVecOrMat, θ::AbstractFloat) = exp.((x .- maximum(x)) * θ);\n",
    "    \n",
    "    #softmax algorithm expects stablized eponentiated e\n",
    "    _sftmax(e::AbstractVecOrMat, d::Integer) = (e ./ sum(e, dims = d));\n",
    "    \n",
    "    _sftmax(_exp(X,theta), dim)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529623d",
   "metadata": {},
   "source": [
    "## Accuracy function\n",
    "The accuracy function takes a matrix of data, that can be the validation set or the test set, and computes the ratio between correct predictions and total examples,as well as the mean probability of assigning the correct label given an example over all the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function accuracy(rbm::SRBM, data::Matrix{T}, labels::Vector{Int}) where T<:AbstractFloat\n",
    "    num_examples = size(data,2);\n",
    "    num_correct  = 0;\n",
    "    mean         = 0.0;\n",
    "    # Define matrix with labels\n",
    "    Identity     = 2.0*Id .- 1.0;\n",
    "    \n",
    "    for i in 1:num_examples\n",
    "        # Compute probability of each label given the data\n",
    "        py_v            = _prob(rbm,[repeat(data[:,i],inner=(1,rbm.num_labels)) ; Identity],collect(1:rbm.num_labels));\n",
    "        \n",
    "        # Predict the label with the highest probability\n",
    "        softy_v         = vec(softmax(py_v,2));\n",
    "        predicted_label = argmax(softy_v);\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        num_correct += 1*(predicted_label == labels[i]);\n",
    "        \n",
    "        # Calculate mean\n",
    "        mean        += softy_v[labels[i]]/num_examples;\n",
    "    end\n",
    "    \n",
    "    return num_correct/num_examples, mean\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a6778",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "Hyperparameter tuning consists in exploring a grid of parameters and training our SRBM with them in order to find the combination that yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using .Iterators\n",
    "\n",
    "function grid_search(\n",
    "        num_visible::Int,\n",
    "        num_labels::Int,\n",
    "        train_data::Matrix{T}, \n",
    "        train_labels::Vector{Int}, \n",
    "        validation_data::Matrix{T}, \n",
    "        validation_labels::Vector{Int}, \n",
    "        test_data::Matrix{T}, \n",
    "        test_labels::Vector{Int}\n",
    ") where T<:AbstractFloat\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rates           = [0.001, 0.005, 0.01, 0.05, 0.1];\n",
    "    weight_decays            = [0.00001, 0.0001, 0.001, 0.01, 0.1];\n",
    "    momentum_coefficients    = [0.0, 0.5, 0.9];\n",
    "    num_hidden_units         = [10, 20, 50, 100];\n",
    "\n",
    "    # Define grid of hyperparameters to search over\n",
    "    hyperparameter_grid      = product(learning_rates, weight_decays, momentum_coefficients, num_hidden_units);\n",
    "\n",
    "    # Train SRBM with each set of hyperparameters and evaluate on validation set\n",
    "    best_validation_accuracy = -Inf;\n",
    "    best_mean_probability    = -Inf;\n",
    "    best_hyperparameters     = (0.0, 0.0, 0.0, 0);\n",
    "    for hyperparameters_set in hyperparameter_grid\n",
    "        learning_rate, weight_decay, momentum_coefficient, num_hidden = hyperparameters_set;\n",
    "        \n",
    "        rbm = SRBM(num_visible = num_visible, num_hidden = num_hidden, num_labels = num_labels); \n",
    "        parameters           = hyperparameters(\n",
    "            learning_rate = learning_rate, \n",
    "            weight_decay = weight_decay, \n",
    "            momentum = momentum_coefficient, \n",
    "            batch_size = 10, \n",
    "            num_epochs = 50\n",
    "        );\n",
    "        \n",
    "        train_srbm(rbm,train_data,train_labels,parameters);\n",
    "        \n",
    "        \n",
    "        validation_accuracy, mean = accuracy(rbm, validation_data, validation_labels)\n",
    "        #println(\"Hyperparameters: \", hyperparameters_set, \", Validation accuracy: \", validation_accuracy)\n",
    "        if validation_accuracy >= best_validation_accuracy && mean > best_mean_probability\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_mean_probability    = mean\n",
    "            best_hyperparameters = hyperparameters_set\n",
    "            #println(\"Hyperparameters: \", hyperparameters_set, \", Validation accuracy: \", validation_accuracy)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return best_hyperparameters;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e29d3",
   "metadata": {},
   "source": [
    "## Training dataset generation\n",
    "We generate the dataset for training, validation and test sets inside a function for memory allocation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function archetype_dataset(\n",
    "        N::Int,\n",
    "        K::Int,\n",
    "        r::T,\n",
    "        MT::Int,\n",
    "        MV::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Calculate probability of keeping bit = 1.\n",
    "    p                 = (r + 1.0)*0.5;\n",
    "    \n",
    "    # Test set corresponds to the random archetypes\n",
    "    test_data         = 2.0*(rand(N,K) .< 0.5) .- 1.0;\n",
    "    test_labels       = collect(1:K);\n",
    "\n",
    "    # Validation set helps tune the hyperparameters with blurred examples (can have different size as training set)\n",
    "    validation_data   = repeat(test_data,inner=(1,MV)).*(2.0*(rand(N,K*MV) .< p) .- 1.0);\n",
    "    validation_labels = repeat(test_labels,inner=MV);\n",
    "\n",
    "    # Training set corresponds to the blurred examples\n",
    "    train_data        = repeat(test_data,inner=(1,MT)).*(2.0*(rand(N,K*MT) .< p) .- 1.0);\n",
    "    train_labels      = repeat(test_labels,inner=MT);\n",
    "    \n",
    "    return train_data, train_labels, validation_data, validation_labels, test_data, test_labels\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c401ccd",
   "metadata": {},
   "source": [
    "## The main code\n",
    "Finally, the main code that will accept the problem variables, as well as the datasets containing training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "function main(\n",
    "        num_visible::Int, \n",
    "        num_hidden::Int, \n",
    "        num_labels::Int, \n",
    "        quality_examples::T,\n",
    "        M_train::Int,\n",
    "        M_validation::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Generate training set\n",
    "    train_data, train_labels, validation_data, validation_labels, test_data, test_labels = archetype_dataset(\n",
    "        num_visible,\n",
    "        num_labels,\n",
    "        quality_examples,\n",
    "        M_train,\n",
    "        M_validation\n",
    "    );\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    #=\n",
    "    learning_rate, weight_decay, momentum, hidden_units = grid_search(\n",
    "        num_visible,\n",
    "        num_labels,\n",
    "        train_data, \n",
    "        train_labels, \n",
    "        validation_data, \n",
    "        validation_labels, \n",
    "        test_data, \n",
    "        test_labels\n",
    "    )\n",
    "    =#\n",
    "    \n",
    "    # Initialize RBM\n",
    "    rbm        = SRBM(\n",
    "        num_visible   = num_visible,\n",
    "        num_hidden    = num_hidden,\n",
    "        num_labels    = num_labels\n",
    "    );\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = hyperparameters(\n",
    "        learning_rate = 0.01,\n",
    "        weight_decay  = 0.00001,\n",
    "        momentum      = 0.9,\n",
    "        batch_size    = 50,\n",
    "        num_epochs    = 1000,\n",
    "        skip          = 10\n",
    "    ) \n",
    "    \n",
    "    train_srbm(rbm,train_data,train_labels,parameters)\n",
    "    \n",
    "    # Export classification probabilities\n",
    "    # Calculate classification probability \n",
    "    ~, P_ARC          = accuracy(rbm,test_data,test_labels);\n",
    "    ~, P_EX           = accuracy(rbm,train_data,train_labels);\n",
    "    \n",
    "    return P_ARC, P_EX;  \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe562f4",
   "metadata": {},
   "source": [
    "## Input loop\n",
    "This code accepts the input arguments *path* and *iteration* in order to compute the *i*-th iteration of the classification probability with respect to the RBM load experiment. The output files are placed in *path*, each containing a matrix with the values of the conditional probability for each combination of *M* (row), *r* and *K* (column).\n",
    "\n",
    "Once again, this could is designed to be sent to a queue system in a computer cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visible      = 200;\n",
    "num_hidden       = 50;\n",
    "\n",
    "Mvec             = [1    2    4    8    16   32  ];\n",
    "rvec             = [0.04 0.06 0.10 0.18 0.32 0.66];\n",
    "Kvec             = [2    4    8    16   32];\n",
    "\n",
    "global const Id  = Matrix{Float64}(I, num_labels, num_labels);\n",
    "\n",
    "M_validation     = 4;\n",
    "\n",
    "probs_archetypes = zeros(length(Mvec),length(rvec)*length(Kvec));\n",
    "probs_examples   = zeros(length(Mvec),length(rvec)*length(Kvec));\n",
    "\n",
    "@assert length(ARGS) == 2\n",
    "path             = ARGS[1];\n",
    "i                = parse(Int,ARGS[2]);\n",
    "\n",
    "for ik in 1:length(Kvec)\n",
    "    for ir in 1:length(rvec)\n",
    "        for im in 1:length(Mvec)\n",
    "            num_labels                = Kvec[ik];\n",
    "            r                         = rvec[ir];\n",
    "            M_train                   = Mvec[im];\n",
    "\n",
    "            P_ARC, P_EX               = main(num_visible,num_hidden,num_labels,r,M_train,M_validation);\n",
    "\n",
    "            col                       = (ik - 1)*length(rvec) + ir;\n",
    "            probs_archetypes[im,col]  = P_ARC;\n",
    "            probs_examples[im,col]    = P_EX;\n",
    "        end;\n",
    "    end\n",
    "end\n",
    "\n",
    "str = @sprintf(\"SRBM-ProbArch-it-%d.txt\",i);\n",
    "writedlm(path*str,probs_archetypes);\n",
    "str = @sprintf(\"SRBM-ProbExam-it-%d.txt\",i);\n",
    "writedlm(path*str,probs_examples);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
