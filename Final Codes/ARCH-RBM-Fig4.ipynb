{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b80bcfb",
   "metadata": {},
   "source": [
    "# Implementation of an Archetype Restricted Boltzmann Machine (ARCH-RBM)\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Printf\n",
    "using Random\n",
    "using DelimitedFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef13e7a",
   "metadata": {},
   "source": [
    "## Definition of structs\n",
    "We define two structs that will help keep the code more organized and easier to maintain. By using a struct for the architecture of the RBM as well as one for the hyperparameters, we can easily pass them as arguments to functions. Additionally, we could define default values for some of the fields, which would make it easier to modify and experiment with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de919c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.@kwdef mutable struct RBM{T<:AbstractFloat}\n",
    "    num_visible   :: Int\n",
    "    num_hidden    :: Int\n",
    "    num_examples  :: Int\n",
    "    W             :: Matrix{T} = 0.01*(randn(num_visible,num_hidden) .- 0.0)\n",
    "    beta_parameter:: T\n",
    "end;\n",
    "\n",
    "Base.@kwdef mutable struct hyperparameters{T<:AbstractFloat}\n",
    "    learning_rate :: T         = 0.001\n",
    "    weight_decay  :: T         = 0.000001\n",
    "    momentum      :: T         = 0.9\n",
    "    batch_size    :: Int       = 100\n",
    "    num_epochs    :: Int       = 1000\n",
    "    CDK           :: Int       = 1\n",
    "    skip          :: Int       = 1\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec8646",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "For neurons that take values $(\\sigma,z) \\in \\{-1,1\\}^{N \\times K}$, the activation function becomes $\\frac{1}{2}(1 + tanh(x))$ instead of the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad36842",
   "metadata": {},
   "outputs": [],
   "source": [
    "function activation(x)\n",
    "    return 0.5*(1.0 + tanh.(x));\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45490e6b",
   "metadata": {},
   "source": [
    "## Expectation function\n",
    "In the case of binary neurons $\\{0,1\\}$ it is the sigmoid, but for binary neurons $\\{-1,1\\}$ it is the hyperbolic tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abe4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "function expectation(x)\n",
    "    return tanh.(x)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e454af6",
   "metadata": {},
   "source": [
    "## Sampling procedures\n",
    "When calculating the gradients, we need to use Gibbs sampling in order to approximate the sum over all the state space. For that, we include a function for sampling any state given its activation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sampleOne(px)\n",
    "    m       = size(px,1);\n",
    "    n       = size(px,2);\n",
    "    ξx      = rand(m,n);\n",
    "    return 2.0*(ξx .<= px) .- 1.0;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33a569",
   "metadata": {},
   "source": [
    "## Calculating the gradients\n",
    "We implement the proposed gradient approximation based on archetype selection with respect to the parameters, and then update them in the direction of the negative gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba013c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function arc_gradients(rbm::RBM, V0::Matrix{T}, H0::Matrix{T}, parameters::hyperparameters) where T<:AbstractFloat\n",
    "    batch_size  = size(V0,2);\n",
    "    \n",
    "    # Positive phase\n",
    "    W_pos       = V0*H0';                             # (Nv,Nh)\n",
    "    \n",
    "    # k-step Contrastive Divergence (CD-k)\n",
    "    VK          = V0;                                 # (Nv,b)\n",
    "    HK          = H0;                                 # (Nh,b)\n",
    "    for _ in 1:parameters.CDK\n",
    "        PH_V    = activation.(rbm.beta_parameter*rbm.W'*VK);       # (Nh,b)\n",
    "        PV_H    = activation.(rbm.beta_parameter*rbm.W*HK);        # (Nv,b)\n",
    "        HK      = sampleOne(PH_V);                    # (Nh,b)\n",
    "        VK      = sampleOne(PV_H);                    # (Nv,b)\n",
    "    end\n",
    "    \n",
    "    # Negative phase\n",
    "    W_neg       = VK*HK';                             # (Nv,Nh)\n",
    "    \n",
    "    # Compute the gradients\n",
    "    dW          = rbm.beta_parameter*(W_pos .- W_neg)./batch_size; # (Nv,Nh)\n",
    "    \n",
    "    return dW;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043e5fb",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "With our function that estimates the gradients, we only need to implement the iterative process over all epochs in which each step looks at a batch of a certain size, computes the gradients with respect to that batch and updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56680365",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainMyData(\n",
    "    rbm::RBM,\n",
    "    TSet::Matrix{T},\n",
    "    Test::Matrix{T},\n",
    "    OSet::Matrix{T},\n",
    "    parameters::hyperparameters,\n",
    ") where T<:AbstractFloat\n",
    "    NTSet       = size(TSet,2);\n",
    "    indices     = collect(1:NTSet);\n",
    "    num_batches = Int(cld(NTSet,parameters.batch_size));\n",
    "    \n",
    "    vW          = zero(rbm.W);                                       #Matrix (Nv,Nh)\n",
    "\n",
    "    for iep in 1:parameters.num_epochs\n",
    "        shuffle!(indices)\n",
    "        \n",
    "        # Initialize the change in parameters\n",
    "        δW      = zero(rbm.W);                                       #Matrix (Nv,Nh)           \n",
    "\n",
    "        for batch in 1:num_batches\n",
    "            # Get indices of batch\n",
    "            batch_indices   = indices[(batch-1)*parameters.batch_size+1:min(batch*parameters.batch_size,NTSet)];\n",
    "            \n",
    "            # Get labels and batches of data\n",
    "            V0              = TSet[:,batch_indices];\n",
    "            H0              = OSet[:,batch_indices];\n",
    "            \n",
    "            δW              = arc_gradients(rbm,V0,H0,parameters);\n",
    "            \n",
    "            # Add L2 regularization to punish large weights\n",
    "            δW             -= parameters.weight_decay*rbm.W;\n",
    "\n",
    "            # Update velocities\n",
    "            vW              = parameters.momentum*vW .+ (1.0 - parameters.momentum)*δW;\n",
    "                \n",
    "            # Update parameters\n",
    "            rbm.W         .+= parameters.learning_rate*vW;\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6193a1",
   "metadata": {},
   "source": [
    "## Useful functions for probability calculation\n",
    "We define some numerically robust functions in order to calculate the marginal probability of a visible state with _prob, and a softmax which we will use for accuracy testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42584e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_sum_exp(u::AbstractVecOrMat, v::AbstractVecOrMat)\n",
    "    maxim(a,b) = (a > b) ? a : b;\n",
    "    max        = maxim.(u,v);\n",
    "    return max + log.(exp.(u - max) + exp.(v - max));\n",
    "end;\n",
    "\n",
    "function _prob(rbm::RBM,x::AbstractVecOrMat)    \n",
    "    return sum(log_sum_exp(rbm.beta_parameter*rbm.W'*x, -rbm.beta_parameter*rbm.W'*x),dims=1);\n",
    "end;\n",
    "\n",
    "function softmax(X::AbstractVecOrMat{T}, dim::Integer, theta::AbstractFloat=1.0)::AbstractVecOrMat where T <: AbstractFloat\n",
    "    #abstract exponentiation function, subtract max for numerical stability and scale by theta\n",
    "    _exp(x::AbstractVecOrMat, theta::AbstractFloat) = exp.((x .- maximum(x)) * theta);\n",
    "    \n",
    "    #softmax algorithm expects stablized eponentiated e\n",
    "    _sftmax(e::AbstractVecOrMat, d::Integer) = (e ./ sum(e, dims = d));\n",
    "    \n",
    "    _sftmax(_exp(X,theta), dim)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8a78c",
   "metadata": {},
   "source": [
    "## Accuracy function\n",
    "The accuracy function takes a matrix of data, that can be the validation set or the test set, and computes the ratio between correct predictions and total examples,as well as the mean probability of assigning the correct label given an example over all the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ea22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function correctHiddenProbability(\n",
    "    TSet::Matrix{Float64},\n",
    "    Test::Matrix{Float64},\n",
    "    rbm ::RBM\n",
    ")\n",
    "    N       = rbm.num_visible;\n",
    "    K       = rbm.num_hidden;\n",
    "    M       = rbm.num_examples;\n",
    "\n",
    "    β       = rbm.beta_parameter;\n",
    "    W       = rbm.W;\n",
    "    \n",
    "    Id      = Matrix(2I,K,K) .- 1;\n",
    "    ID      = repeat(Id,inner=(M,1));\n",
    "    \n",
    "    log_num = zeros(K);\n",
    "    log_den = zeros(K*M);\n",
    "    \n",
    "    for i in 1:K\n",
    "        ξ              = Test[:,i];\n",
    "        Z              = Id[:,i];\n",
    "        sum_exps       = _prob(rbm,ξ);\n",
    "        log_num[i]     = β*(ξ'*W*Z) - sum_exps[1];\n",
    "        \n",
    "        for j in 1:M\n",
    "            k          = M*(i - 1) + j;\n",
    "            η          = TSet[:,k];\n",
    "            sum_exps   = _prob(rbm,η);\n",
    "            log_den[k] = β*(η'*W*Z) - sum_exps[1];\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    log_num = repeat(log_num, inner=(M,1));\n",
    "    \n",
    "    ratio   = log_num - log_den;\n",
    "    ratio   = exp.(ratio);\n",
    "    \n",
    "    return exp.(log_num), exp.(log_den), sum(ratio)/length(ratio);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec2337",
   "metadata": {},
   "source": [
    "## Overlap function\n",
    "We need a function that calculates the overlap between the expected value of the visible layer when fixing a label and the original archetype related to that label. It also calculates the overlap between the same expected visible configuration and the distorted examples so that we can compare both values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dad2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function calculateOverlap(\n",
    "    TSet::Matrix{Float64},\n",
    "    Test::Matrix{Float64},\n",
    "    rbm ::RBM\n",
    ")\n",
    "    N       = rbm.num_visible;\n",
    "    K       = rbm.num_hidden;\n",
    "    M       = rbm.num_examples;\n",
    "    Id      = Matrix(2I,K,K) .- 1;\n",
    "\n",
    "    # 1: Start with Id{-1,1} of size K x K\n",
    "    H       = Id;\n",
    "    A       = rbm.W*H*rbm.beta_parameter;\n",
    "    # 2: Calculate the expected value of σ over p(σ|z(μ)) (Same as sampling infinite times -> whole state space)\n",
    "    V_M     = mapslices(x -> expectation(x),A,dims=1);\n",
    "    # 3: Calculate avg of V_M and V_N\n",
    "    V_N     = repeat(V_M, inner=(1,M));\n",
    "\n",
    "    # 4: Calculate <m> and <n>\n",
    "    m = 1/N*[dot(V_M[:,i],Test[:,i]) for i in 1:K];\n",
    "    n = 1/N*[dot(V_N[:,i],TSet[:,i]) for i in 1:K*M];\n",
    "\n",
    "    return sum(m)/K, sum(n)/(K*M);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7949d63",
   "metadata": {},
   "source": [
    "## Training dataset generation\n",
    "We generate the dataset for training, validation and test sets inside a function for memory allocation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function archetype_dataset(\n",
    "        N::Int,\n",
    "        K::Int,\n",
    "        r::T,\n",
    "        M::Int\n",
    ") where T<:AbstractFloat\n",
    "    \n",
    "    # Calculate probability of keeping bit = 1.\n",
    "    p                 = (r + 1.0)*0.5;\n",
    "    \n",
    "    # Test set corresponds to the random archetypes\n",
    "    Test              = 2.0*(rand(N,K) .< 0.5) .- 1.0;\n",
    "    Id                = Matrix(2I,K,K) .- 1.0;\n",
    "\n",
    "    # Training set corresponds to the blurred examples\n",
    "    TSet              = repeat(Test,inner=(1,M)).*(2.0*(rand(N,K*M) .< p) .- 1.0);\n",
    "    OSet              = repeat(Id,inner=(1,M));\n",
    "    \n",
    "    return TSet, Test, OSet\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb47fa",
   "metadata": {},
   "source": [
    "## The main code\n",
    "The main code that will accept the problem variables, as well as the datasets containing training, validation and test sets. It calculates and prints both the classification accuracy and the overlaps for the training set (blurred examples) and the test set (archetypes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function main(\n",
    "        num_visible::Int, \n",
    "        num_hidden::Int, \n",
    "        num_examples::Int,\n",
    "        quality_examples::T;\n",
    "        beta_parameter::T=1.0,\n",
    "        learning_rate::T=0.001,\n",
    "        weight_decay::T=0.00001\n",
    ") where T<:AbstractFloat\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = hyperparameters(\n",
    "        learning_rate  = learning_rate,\n",
    "        weight_decay   = weight_decay,\n",
    "        momentum       = 0.9,\n",
    "        batch_size     = 50,\n",
    "        num_epochs     = 5000,\n",
    "        skip           = 10\n",
    "    ) \n",
    "\n",
    "    # Initialize RBM\n",
    "    rbm        = RBM(\n",
    "        num_visible    = num_visible,\n",
    "        num_hidden     = num_hidden,\n",
    "        num_examples   = num_examples,\n",
    "        beta_parameter = beta_parameter\n",
    "    );\n",
    "    \n",
    "    # Generate training set\n",
    "    TSet, Test, OSet = archetype_dataset(\n",
    "        num_visible,\n",
    "        num_hidden,\n",
    "        quality_examples,\n",
    "        num_examples\n",
    "    );\n",
    "    \n",
    "    #display(Test)\n",
    "    #display(TSet)\n",
    "\n",
    "    # CD-k for all vectors in the training set\n",
    "\n",
    "    #println(\"Now training RBM...\")\n",
    "    \n",
    "    trainMyData(rbm,TSet,Test,OSet,parameters);\n",
    "    \n",
    "    #println(\"Done.\")\n",
    "    \n",
    "    \n",
    "    P_ARC, P_EX, PZ     = correctHiddenProbability(TSet,Test,rbm);\n",
    "    avg_m, avg_n        = calculateOverlap(TSet,Test,rbm);\n",
    "    return rbm, P_ARC, P_EX, avg_m, avg_n;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad8fc4",
   "metadata": {},
   "source": [
    "## Classification probabilities with respect to the load of the RBM\n",
    "\n",
    "This time, we define the number of visible units $N$ and the ranges in which the number of hidden units (archetypes) $K$, the number of examples per archetype $M$ and their quality $r$ will vary.\n",
    "\n",
    "Then, we nest the loop with a hierarchy $K$ first, then $r$ and finally $M$. This does not affect the result in any way, this is just to organize better the output results.\n",
    "\n",
    "To use this code, it must be called using a terminal with the inputs $path$ and $i$.\n",
    "\n",
    "This code outputs two .txt files containing the classification probabilities for a single iteration. Since the code runs for a very long time, it is better to send this code multiple times with different $i$ to a queue system in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30594c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200;\n",
    "Mvec = [1    2    4    8    16   32  ];\n",
    "rvec = [0.04 0.06 0.10 0.18 0.32 0.66];\n",
    "Kvec = [2    4    8    16   32];\n",
    "\n",
    "avg_m_M = zeros(length(Mvec),length(rvec)*length(Kvec));\n",
    "avg_n_M = zeros(length(Mvec),length(rvec)*length(Kvec));\n",
    "\n",
    "\n",
    "@assert length(ARGS) == 2\n",
    "path    = ARGS[1];\n",
    "i       = parse(Int,ARGS[2]);\n",
    "\n",
    "for ik in 1:length(Kvec)\n",
    "    for ir in 1:length(rvec)\n",
    "        for im in 1:length(Mvec)\n",
    "            K                = Kvec[ik];\n",
    "            r                = rvec[ir];\n",
    "            M                = Mvec[im];\n",
    "\n",
    "            rbm, P_ARC, P_EX = main(N,K,M,r;beta_parameter=1/200,learning_rate=0.1,weight_decay=0.0005);\n",
    "\n",
    "            col              = (ik - 1)*length(rvec) + ir;\n",
    "            avg_m_M[im,col]  = P_ARC;\n",
    "            avg_n_M[im,col]  = P_EX;\n",
    "        end;\n",
    "    end\n",
    "end\n",
    "\n",
    "str = @sprintf(\"ProbArch-it-%d.txt\",i);\n",
    "writedlm(path*str,avg_m_M);\n",
    "str = @sprintf(\"ProbExam-it-%d.txt\",i);\n",
    "writedlm(path*str,avg_n_M);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
